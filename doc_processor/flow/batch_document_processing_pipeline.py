#!/usr/bin/env python3
"""
ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
- í´ë”ì˜ ëª¨ë“  PDF íŒŒì¼ì„ ì¼ê´„ ì²˜ë¦¬
- ê¸°ì¡´ document_processing_pipelineì˜ ë‚´ë¶€ íƒœìŠ¤í¬ë“¤ì„ ì§ì ‘ ì‚¬ìš©
"""

import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

# ì„¤ì • ë° ë°ì´í„°ë² ì´ìŠ¤
from config import config
from database import Document, get_db_session

# ê¸°ì¡´ document_processing_pipelineì˜ íƒœìŠ¤í¬ë“¤ import
from document_processing_pipeline import (
    capture_page_images,
    complete_processing_job,
    create_document_metadata,
    create_processing_job,
    create_vector_database,
    extract_text_from_document,
    generate_image_descriptions,
    initialize_database,
    save_document_chunk,
    update_document_processing_status,
)
from prefect import flow, get_run_logger, task
from prefect.context import get_run_context
from prefect.task_runners import ConcurrentTaskRunner

# Vector DB (Milvus)
from pymilvus import Collection, connections

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@task(name="í´ë”_PDF_íŒŒì¼_ê²€ìƒ‰")
def find_pdf_files(folder_path: str) -> List[str]:
    """í´ë”ì—ì„œ ëª¨ë“  PDF íŒŒì¼ì„ ì°¾ìŠµë‹ˆë‹¤"""
    logger = get_run_logger()
    
    folder = Path(folder_path)
    if not folder.exists():
        raise FileNotFoundError(f"í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
    
    if not folder.is_dir():
        raise ValueError(f"ê²½ë¡œê°€ í´ë”ê°€ ì•„ë‹™ë‹ˆë‹¤: {folder_path}")
    
    # PDF íŒŒì¼ ê²€ìƒ‰ (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
    pdf_files = []
    for ext in ['*.pdf', '*.PDF']:
        pdf_files.extend(folder.glob(ext))
    
    # ì¬ê·€ì  ê²€ìƒ‰ (í•˜ìœ„ í´ë”ë„ í¬í•¨)
    for ext in ['**/*.pdf', '**/*.PDF']:
        pdf_files.extend(folder.glob(ext))
    
    # ì¤‘ë³µ ì œê±° ë° ë¬¸ìì—´ë¡œ ë³€í™˜
    pdf_paths = sorted(list(set([str(f) for f in pdf_files])))
    
    logger.info(f"ğŸ“‚ í´ë” '{folder_path}'ì—ì„œ {len(pdf_paths)}ê°œì˜ PDF íŒŒì¼ ë°œê²¬")
    for i, pdf_path in enumerate(pdf_paths, 1):
        logger.info(f"   {i}. {Path(pdf_path).name}")
    
    return pdf_paths

@task(name="íŒŒì¼_í¬ê¸°_í•„í„°ë§")
def filter_files_by_size(pdf_files: List[str], max_size_mb: float = 50.0) -> List[str]:
    """íŒŒì¼ í¬ê¸°ë¡œ í•„í„°ë§ (ë„ˆë¬´ í° íŒŒì¼ ì œì™¸)"""
    logger = get_run_logger()
    
    filtered_files = []
    max_size_bytes = max_size_mb * 1024 * 1024
    
    for pdf_file in pdf_files:
        file_path = Path(pdf_file)
        if file_path.exists():
            file_size = file_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            
            if file_size <= max_size_bytes:
                filtered_files.append(pdf_file)
                logger.info(f"âœ… {file_path.name} ({file_size_mb:.1f}MB) - ì²˜ë¦¬ ëŒ€ìƒ")
            else:
                logger.warning(f"âš ï¸ {file_path.name} ({file_size_mb:.1f}MB) - í¬ê¸° ì´ˆê³¼ë¡œ ì œì™¸")
        else:
            logger.error(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {pdf_file}")
    
    logger.info(f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(filtered_files)}/{len(pdf_files)} íŒŒì¼ì´ ì²˜ë¦¬ ëŒ€ìƒ")
    return filtered_files

@task(name="ë‹¨ì¼_ë¬¸ì„œ_ì™„ì „_ì²˜ë¦¬")
def process_single_document_complete(document_path: str, max_pages: int = None, skip_image_processing: bool = False, document_type: str = 'common') -> Dict[str, Any]:
    """ë‹¨ì¼ ë¬¸ì„œì˜ ì „ì²´ ì²˜ë¦¬ ê³¼ì •ì„ ì‹¤í–‰í•˜ëŠ” íƒœìŠ¤í¬"""
    logger = get_run_logger()
    
    try:
        logger.info(f"ğŸš€ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {Path(document_path).name}")
        
        # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path(document_path).exists():
            raise FileNotFoundError(f"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
        
        # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
        if not config.validate_config():
            raise ValueError("í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        # 0ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        logger.info("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”")
        db_initialized = initialize_database()
        if not db_initialized:
            logger.warning("âš ï¸ PostgreSQL ì—°ê²° ì‹¤íŒ¨, ë©”íƒ€ë°ì´í„° ì €ì¥ ì—†ì´ ì§„í–‰")
        
        # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
        doc_metadata = None
        job_id = None
        if db_initialized:
            try:
                doc_metadata = create_document_metadata(document_path, document_type)
                flow_run_context = get_run_context()
                flow_run_id = str(flow_run_context.flow_run.id) if flow_run_context and flow_run_context.flow_run else "batch_run"
                job_id = create_processing_job(doc_metadata["doc_id"], flow_run_id)
                logger.info(f"ğŸ“‹ ë¬¸ì„œ ID: {doc_metadata['doc_id']}")
                
                # ì´ë¯¸ ì™„ë£Œëœ ë¬¸ì„œì¸ì§€ í™•ì¸
                if not doc_metadata["is_new"]:
                    logger.info(f"â­ï¸ ì´ë¯¸ ì™„ë£Œëœ ë¬¸ì„œ ê±´ë„ˆë›°ê¸°: {Path(document_path).name}")
                    return {
                        "document_path": document_path,
                        "status": "skipped",
                        "reason": "already_completed",
                        "doc_id": doc_metadata["doc_id"]
                    }
            except Exception as e:
                logger.warning(f"âš ï¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")
                db_initialized = False
        
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if job_id:
            update_job_progress(job_id, "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘", 0)
            
        logger.info("ğŸ“„ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        text_result = extract_text_from_document(document_path, max_pages)
        
        if job_id:
            update_job_progress(job_id, f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ - {text_result['total_pages']}í˜ì´ì§€", 1,
                              {"extracted_pages": text_result['total_pages']})
        
        if skip_image_processing:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
            logger.info("â­ï¸ 2-3ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°")
            image_result = {"image_paths": []}
            description_result = {"image_descriptions": {}, "total_images": 0}
            
            if job_id:
                update_job_progress(job_id, "ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°", 3)
                
        else:
            # 2ë‹¨ê³„: í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜
            if job_id:
                update_job_progress(job_id, "í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜ ì‹œì‘", 1)
                
            logger.info("ğŸ–¼ï¸ 2ë‹¨ê³„: í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜")
            image_result = capture_page_images(document_path, max_pages=max_pages)
            
            if job_id:
                update_job_progress(job_id, f"ì´ë¯¸ì§€ ìº¡ì²˜ ì™„ë£Œ - {len(image_result['image_paths'])}ê°œ", 2,
                                  {"captured_images": len(image_result['image_paths'])})
            
            # 3ë‹¨ê³„: GPTë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
            if job_id:
                update_job_progress(job_id, "GPT ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹œì‘", 2)
                
            logger.info("ğŸ¤– 3ë‹¨ê³„: GPT ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±")
            description_result = generate_image_descriptions(image_result["image_paths"])
            
            if job_id:
                update_job_progress(job_id, f"GPT ì„¤ëª… ìƒì„± ì™„ë£Œ - {description_result['total_images']}ê°œ", 3,
                                  {"generated_descriptions": description_result['total_images']})
        
        # 4ë‹¨ê³„: Vector DB êµ¬ì„±
        if job_id:
            update_job_progress(job_id, "Vector DB êµ¬ì„± ì‹œì‘ (ì„ë² ë”© ìƒì„±)", 3)
            
        logger.info("ğŸ—„ï¸ 4ë‹¨ê³„: Vector DB êµ¬ì„±")
        vector_result = create_vector_database(
            text_result, 
            description_result, 
            document_path
        )
        
        if job_id:
            update_job_progress(job_id, f"Vector DB êµ¬ì„± ì™„ë£Œ - {vector_result['total_documents']}ê°œ ë²¡í„°", 4,
                              {"vector_documents": vector_result['total_documents'],
                               "embedding_model": vector_result['embedding_model']})
        
        # 5ë‹¨ê³„: PostgreSQLì— ì²­í¬ ë°ì´í„° ì €ì¥
        saved_chunks = 0
        if db_initialized and doc_metadata and vector_result.get("total_documents", 0) > 0:
            logger.info("ğŸ’¾ 5ë‹¨ê³„: PostgreSQLì— ì²­í¬ ë°ì´í„° ì €ì¥")
            try:
                # Milvusì—ì„œ ë°©ê¸ˆ ì‚½ì…ëœ ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ PostgreSQLì— ì €ì¥
                connections.connect("default", uri=config.MILVUS_URI)
                collection = Collection(config.MILVUS_COLLECTION_NAME)
                collection.load()
                
                # í˜„ì¬ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ë°ì´í„° ì¡°íšŒ
                results = collection.query(
                    expr=f'document_path == "{document_path}"',
                    output_fields=["id", "document_path", "page_number", "content_type", 
                                 "content", "text_content", "image_description", "image_path"]
                )
                
                for doc_data in results:
                    chunk_data = {
                        "content_type": doc_data.get("content_type", "combined"),
                        "content": doc_data.get("content", ""),
                        "text_content": doc_data.get("text_content", ""),
                        "image_description": doc_data.get("image_description", ""),
                        "image_path": doc_data.get("image_path", ""),
                        "milvus_id": str(doc_data.get("id", ""))
                    }
                    
                    save_document_chunk(
                        doc_metadata["doc_id"], 
                        doc_data.get("page_number", 0), 
                        chunk_data
                    )
                    saved_chunks += 1
                    
                # ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                update_document_processing_status(
                    doc_metadata["doc_id"], 
                    "completed",
                    total_pages=text_result['total_pages'],
                    processed_pages=text_result['total_pages'],
                    vector_count=vector_result['total_documents']
                )
                
                # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
                if job_id:
                    complete_processing_job(job_id, saved_chunks, vector_result['total_documents'])
                    
                logger.info(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ: {saved_chunks}ê°œ ì²­í¬")
                
            except Exception as e:
                logger.error(f"âŒ PostgreSQL ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                if doc_metadata and job_id:
                    try:
                        update_document_processing_status(doc_metadata["doc_id"], "failed", error_log=str(e))
                        complete_processing_job(job_id, saved_chunks, vector_result['total_documents'], str(e))
                    except:
                        pass
        
        # ì„±ê³µ ê²°ê³¼ ë°˜í™˜
        result = {
            "document_path": document_path,
            "status": "success",
            "doc_id": doc_metadata.get("doc_id") if doc_metadata else None,
            "total_pages": text_result['total_pages'],
            "captured_images": len(image_result['image_paths']),
            "generated_descriptions": description_result['total_images'],
            "vector_documents": vector_result['total_documents'],
            "saved_chunks": saved_chunks,
            "processing_time": datetime.now().isoformat()
        }
        
        logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {Path(document_path).name}")
        logger.info(f"   ğŸ“Š í˜ì´ì§€: {result['total_pages']}, ì´ë¯¸ì§€: {result['captured_images']}, ë²¡í„°: {result['vector_documents']}")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: {Path(document_path).name} - {str(e)}")
        
        # ì‹¤íŒ¨ ì‹œ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        if db_initialized and doc_metadata and job_id:
            try:
                update_document_processing_status(doc_metadata["doc_id"], "failed", error_log=str(e))
                complete_processing_job(job_id, 0, 0, str(e))
            except:
                pass
        
        return {
            "document_path": document_path,
            "status": "failed",
            "error": str(e),
            "failure_time": datetime.now().isoformat()
        }

@flow(
    name="batch_document_processing_pipeline",
    description="í´ë”ì˜ ëª¨ë“  PDF íŒŒì¼ì„ ì¼ê´„ ì²˜ë¦¬í•˜ëŠ” ë°°ì¹˜ íŒŒì´í”„ë¼ì¸",
    task_runner=ConcurrentTaskRunner(max_workers=2)  # ë™ì‹œ ì²˜ë¦¬ íŒŒì¼ ìˆ˜ ì œí•œ
)
def batch_document_processing_pipeline(
    folder_path: str,
    max_pages: int = None,
    max_file_size_mb: float = 50.0,
    skip_existing: bool = True
):
    """
    í´ë” ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        folder_path: ì²˜ë¦¬í•  í´ë” ê²½ë¡œ
        max_pages: ê° ë¬¸ì„œë‹¹ ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        max_file_size_mb: ì²˜ë¦¬í•  ìµœëŒ€ íŒŒì¼ í¬ê¸° (MB)
        skip_existing: ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ ê±´ë„ˆë›°ê¸° ì—¬ë¶€
    """
    logger = get_run_logger()
    logger.info(f"ğŸ“ ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {folder_path}")
    
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    if not config.validate_config():
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    start_time = datetime.now()
    
    # 1ë‹¨ê³„: PDF íŒŒì¼ ê²€ìƒ‰
    logger.info("ğŸ” 1ë‹¨ê³„: PDF íŒŒì¼ ê²€ìƒ‰")
    pdf_files = find_pdf_files(folder_path)
    
    if not pdf_files:
        logger.warning(f"âš ï¸ '{folder_path}' í´ë”ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "folder_path": folder_path,
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "status": "no_files_found",
            "start_time": start_time.isoformat(),
            "end_time": datetime.now().isoformat()
        }
    
    # 2ë‹¨ê³„: íŒŒì¼ í¬ê¸° í•„í„°ë§
    logger.info("ğŸ“ 2ë‹¨ê³„: íŒŒì¼ í¬ê¸° í•„í„°ë§")
    filtered_files = filter_files_by_size(pdf_files, max_file_size_mb)
    
    if not filtered_files:
        logger.warning("âš ï¸ í¬ê¸° ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {
            "folder_path": folder_path,
            "total_files": len(pdf_files),
            "processed_files": 0,
            "failed_files": 0,
            "status": "no_valid_files",
            "start_time": start_time.isoformat(),
            "end_time": datetime.now().isoformat()
        }
    
    # 3ë‹¨ê³„: ë°°ì¹˜ ì²˜ë¦¬ (ë™ì‹œ ì²˜ë¦¬)
    logger.info(f"âš¡ 3ë‹¨ê³„: {len(filtered_files)}ê°œ íŒŒì¼ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘")
    
    # ê° íŒŒì¼ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬ (ì‹¤ì œ íƒœìŠ¤í¬ë“¤ ì§ì ‘ ì‹¤í–‰)
    processing_futures = []
    for pdf_file in filtered_files:
        future = process_single_document_complete.submit(
            pdf_file, 
            max_pages=max_pages,
            skip_image_processing=False
        )
        processing_futures.append(future)
    
    # ëª¨ë“  ì²˜ë¦¬ ì™„ë£Œ ëŒ€ê¸°
    processing_results = [future.result() for future in processing_futures]
    
    # ê²°ê³¼ ì§‘ê³„
    successful_files = [r for r in processing_results if r["status"] == "success"]
    failed_files = [r for r in processing_results if r["status"] == "failed"]
    skipped_files = [r for r in processing_results if r["status"] == "skipped"]
    
    end_time = datetime.now()
    total_duration = (end_time - start_time).total_seconds()
    
    # ìµœì¢… ê²°ê³¼
    batch_result = {
        "folder_path": folder_path,
        "total_files_found": len(pdf_files),
        "total_files_processed": len(filtered_files),
        "successful_files": len(successful_files),
        "failed_files": len(failed_files),
        "skipped_files": len(skipped_files),
        "processing_results": processing_results,
        "start_time": start_time.isoformat(),
        "end_time": end_time.isoformat(),
        "total_duration_seconds": total_duration,
        "status": "completed",
        "settings": {
            "max_pages": max_pages,
            "max_file_size_mb": max_file_size_mb,
            "max_concurrent_workers": 2,
            "skip_existing": skip_existing
        }
    }
    
    # ìƒì„¸ í†µê³„ ê³„ì‚°
    total_pages_processed = sum(r.get("total_pages", 0) for r in successful_files)
    total_vectors_created = sum(r.get("vector_documents", 0) for r in successful_files)
    total_chunks_saved = sum(r.get("saved_chunks", 0) for r in successful_files)
    
    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    logger.info("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ìš”ì•½:")
    logger.info(f"   - ì´ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    logger.info(f"   - ì²˜ë¦¬ ëŒ€ìƒ: {len(filtered_files)}ê°œ")
    logger.info(f"   - ì„±ê³µ: {len(successful_files)}ê°œ")
    logger.info(f"   - ì‹¤íŒ¨: {len(failed_files)}ê°œ")
    logger.info(f"   - ê±´ë„ˆëœ€: {len(skipped_files)}ê°œ (ì´ë¯¸ ì™„ë£Œëœ ë¬¸ì„œ)")
    logger.info(f"   - ì´ í˜ì´ì§€: {total_pages_processed}í˜ì´ì§€")
    logger.info(f"   - ì´ ë²¡í„°: {total_vectors_created}ê°œ")
    logger.info(f"   - ì´ ì²­í¬: {total_chunks_saved}ê°œ")
    logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_duration:.1f}ì´ˆ")
    
    if successful_files:
        logger.info("âœ… ì„±ê³µí•œ íŒŒì¼ë“¤:")
        for success in successful_files:
            logger.info(f"   - {Path(success['document_path']).name}: "
                       f"{success['total_pages']}í˜ì´ì§€, {success['vector_documents']}ë²¡í„°")
    
    if skipped_files:
        logger.info("â­ï¸ ê±´ë„ˆë›´ íŒŒì¼ë“¤:")
        for skipped in skipped_files:
            logger.info(f"   - {Path(skipped['document_path']).name}: {skipped.get('reason', 'unknown')}")
    
    if failed_files:
        logger.warning("âŒ ì‹¤íŒ¨í•œ íŒŒì¼ë“¤:")
        for failed in failed_files:
            logger.warning(f"   - {Path(failed['document_path']).name}: {failed['error']}")
    
    # ìƒì„¸ ê²°ê³¼ë„ batch_resultì— ì¶”ê°€
    batch_result.update({
        "detailed_stats": {
            "total_pages_processed": total_pages_processed,
            "total_vectors_created": total_vectors_created,
            "total_chunks_saved": total_chunks_saved
        }
    })
    
    return batch_result

# ===============================
# ì‹¤í–‰ ì˜ˆì œ
# ===============================
if __name__ == "__main__":
    # ì„¤ì • í™•ì¸
    config.print_config()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
    import os
    test_folder = os.getenv("TEST_FOLDER_PATH", "./test_folder")
    
    print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ í´ë” ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {test_folder}")
    result = batch_document_processing_pipeline(
        folder_path=test_folder,
        max_pages=3,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 3í˜ì´ì§€ë§Œ
        max_file_size_mb=100.0
    )
    print(f"ğŸ¯ ë°°ì¹˜ ì²˜ë¦¬ ê²°ê³¼: {result['status']}")
    print(f"   ì„±ê³µ: {result['successful_files']}/{result['total_files_processed']} íŒŒì¼")
