#!/usr/bin/env python3
"""
4ë‹¨ê³„ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
1. ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Azure AI Search)
2. í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜ ë° ì €ì¥
3. GPTë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
4. í…ìŠ¤íŠ¸ì™€ ì„¤ëª…ì„ í•©ì³ì„œ Vector DB êµ¬ì„± (Azure OpenAI ì„ë² ë”© ì‚¬ìš©)
"""

import asyncio
import base64
import io
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import fitz  # PyMuPDF

# Azure OpenAI (í†µí•© openai íŒ¨í‚¤ì§€ ì‚¬ìš©)
import openai

# í™˜ê²½ ì„¤ì •
from config import config

# ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)
from database import db_manager, get_db_session
from pdf2image import convert_from_path
from PIL import Image
from prefect import flow, get_run_logger, task
from prefect.futures import PrefectFuture
from prefect.task_runners import ConcurrentTaskRunner

# Vector DB (Milvus)
from pymilvus import (
    Collection,
    CollectionSchema,
    DataType,
    FieldSchema,
    connections,
    utility,
)

from shared_core import (
    Document,
    DocumentChunk,
    DocumentChunkService,
    DocumentService,
    ProcessingJob,
    ProcessingJobService,
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===============================
# PostgreSQL ê´€ë ¨ íƒœìŠ¤í¬
# ===============================

@task(name="ì´ˆê¸°í™”_ë°ì´í„°ë² ì´ìŠ¤_ì—°ê²°")
def initialize_database():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™”"""
    logger = get_run_logger()
    try:
        db_manager.initialize()
        if db_manager.test_connection():
            logger.info("âœ… PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ")
            return True
        else:
            logger.error("âŒ PostgreSQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
            return False
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return False

@task(name="ìƒì„±_ë¬¸ì„œ_ë©”íƒ€ë°ì´í„°")
def create_document_metadata(document_path: str, document_type: str = 'common') -> Dict[str, Any]:
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± ë° ì €ì¥ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)"""
    logger = get_run_logger()
    
    try:
        # ê³µí†µ ëª¨ë“ˆì˜ DocumentService ì‚¬ìš©
        with next(get_db_session()) as session:
            doc_service = DocumentService(session)
            
            # ë¬¸ì„œ ìƒì„± (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
            result = doc_service.create_document_from_path(
                file_path=document_path,
                user_id="system",  # ì‹œìŠ¤í…œ ì‚¬ìš©ì
                document_type=document_type,
                is_public=True  # Prefectì—ì„œ ì²˜ë¦¬í•˜ëŠ” ë¬¸ì„œëŠ” ê³µê°œ
            )
            
            logger.info(f"âœ… ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± ì™„ë£Œ: {result['document_id']}")
            return {
                "doc_id": result["document_id"],
                "doc_name": result["document_name"],
                "file_size": result["file_size"],
                "file_type": result["file_type"],
                "file_hash": result["file_hash"],
                "status": result["status"]
            }
            
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

@task(name="ìƒì„±_ì²˜ë¦¬_ì‘ì—…_ë¡œê·¸")
def create_processing_job(doc_id: str, flow_run_id: str) -> str:
    """ì²˜ë¦¬ ì‘ì—… ë¡œê·¸ ìƒì„± (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)"""
    logger = get_run_logger()
    
    try:
        with next(get_db_session()) as session:
            job_service = ProcessingJobService(session)
            
            result = job_service.create_job(
                doc_id=doc_id,
                job_type="document_processing",
                flow_run_id=flow_run_id,
                total_steps=4  # í…ìŠ¤íŠ¸ ì¶”ì¶œ, ì´ë¯¸ì§€ ìº¡ì²˜, GPT ì„¤ëª…, Vector DB
            )
            
            logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ë¡œê·¸ ìƒì„±: {result['job_id']}")
            return result["job_id"]
            
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì‘ì—… ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

@task(name="ì €ì¥_ë¬¸ì„œ_ì²­í¬")
def save_document_chunk(doc_id: str, page_number: int, chunk_data: Dict[str, Any]) -> str:
    """ë¬¸ì„œ ì²­í¬ ì •ë³´ë¥¼ PostgreSQLì— ì €ì¥ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)"""
    logger = get_run_logger()
    
    try:
        with next(get_db_session()) as session:
            chunk_service = DocumentChunkService(session)
            
            result = chunk_service.create_chunk(
                doc_id=doc_id,
                page_number=page_number,
                chunk_type=chunk_data.get("content_type", "unknown"),
                content=chunk_data.get("content", ""),
                image_description=chunk_data.get("image_description", ""),
                image_path=chunk_data.get("image_path", ""),
                milvus_id=chunk_data.get("milvus_id", ""),
                embedding_model="text-embedding-3-large",
                vector_dimension=3072,
                metadata_json={
                    "processing_timestamp": datetime.utcnow().isoformat(),
                    "original_data": chunk_data
                }
            )
            
            logger.info(f"âœ… ë¬¸ì„œ ì²­í¬ ì €ì¥: {result['chunk_id']}")
            return result["chunk_id"]
            
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
        raise

@task(name="ì—…ë°ì´íŠ¸_ë¬¸ì„œ_ì²˜ë¦¬_ìƒíƒœ")
def update_document_processing_status(doc_id: str, status: str, **kwargs):
    """ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê³µí†µ ëª¨ë“ˆ ì‚¬ìš©)"""
    logger = get_run_logger()
    
    try:
        with next(get_db_session()) as session:
            doc_service = DocumentService(session)
            
            success = doc_service.update_document_processing_status(
                document_id=doc_id,
                status=status,
                **kwargs
            )
            
            if success:
                logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸: {doc_id} -> {status}")
            else:
                logger.warning(f"âš ï¸ ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {doc_id}")
            
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise

@task(name="ì—…ë°ì´íŠ¸_ì‘ì—…_ì§„í–‰ë¥ ")
def update_job_progress(job_id: str, current_step: str, completed_steps: int = None, additional_data: dict = None):
    """ì‘ì—… ì§„í–‰ë¥  ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸"""
    logger = get_run_logger()
    
    try:
        with next(get_db_session()) as session:
            job_service = ProcessingJobService(session)
            
            # ì—…ë°ì´íŠ¸í•  ë°ì´í„° ì¤€ë¹„
            update_data = {
                "current_step": current_step,
                "updated_at": datetime.utcnow()
            }
            
            if completed_steps is not None:
                update_data["completed_steps"] = completed_steps
            
            if additional_data:
                update_data.update(additional_data)
            
            success = job_service.update_job_status(job_id, "running", **update_data)
            
            if success:
                progress_percent = 0
                if completed_steps is not None:
                    # total_stepsëŠ” 4ë¡œ ê³ ì • (í…ìŠ¤íŠ¸ ì¶”ì¶œ, ì´ë¯¸ì§€ ìº¡ì²˜, GPT ì„¤ëª…, Vector DB)
                    progress_percent = (completed_steps / 4) * 100
                logger.info(f"ğŸ“Š ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸: {job_id} - {current_step} ({progress_percent:.0f}%)")
            else:
                logger.warning(f"âš ï¸ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {job_id}")
            
            return success
            
    except Exception as e:
        logger.error(f"âŒ ì‘ì—… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ê°€ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì¤‘ë‹¨ì‹œí‚¤ì§€ ì•Šë„ë¡ ì˜ˆì™¸ë¥¼ ì‚¼í‚´
        return False

@task(name="ì™„ë£Œ_ì²˜ë¦¬_ì‘ì—…")
def complete_processing_job(job_id: str, success_count: int, total_count: int, error_message: str = None):
    """ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ"""
    logger = get_run_logger()
    
    try:
        with next(get_db_session()) as session:
            job = session.query(ProcessingJob).filter_by(job_id=job_id).first()
            if not job:
                raise ValueError(f"ì²˜ë¦¬ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {job_id}")
            
            # ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸
            job.status = "completed" if error_message is None else "failed"
            job.completed_at = datetime.utcnow()
            job.completed_steps = 4  # ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ
            job.current_step = "ì™„ë£Œ" if error_message is None else f"ì‹¤íŒ¨: {error_message}"
            
            # ê²°ê³¼ ë°ì´í„° ì €ì¥
            job.result_data = {
                "total_chunks": total_count,
                "successful_chunks": success_count,
                "failed_chunks": total_count - success_count,
                "duration_seconds": int((datetime.utcnow() - job.started_at).total_seconds()),
                "completion_time": datetime.utcnow().isoformat()
            }
            
            if error_message:
                job.error_message = error_message
            
            session.commit()
            logger.info(f"âœ… ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ: {job_id} - ì„±ê³µ: {success_count}/{total_count} ì²­í¬")
            
    except Exception as e:
        logger.error(f"âŒ ì²˜ë¦¬ ì‘ì—… ì™„ë£Œ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        raise

# ===============================
# Azure OpenAI ì„ë² ë”© í•¨ìˆ˜ (ë³„ë„ API ë²„ì „ ì‚¬ìš©)
# ===============================
def get_azure_openai_embedding(text: str) -> List[float]:
    """Azure OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. (ì„ë² ë”© ì „ìš© API ë²„ì „)"""
    try:
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ìƒˆë¡œìš´ API ë°©ì‹)
        client = openai.AzureOpenAI(
            azure_endpoint=config.AZURE_OPENAI_ENDPOINT,
            api_key=config.AZURE_OPENAI_KEY,
            api_version=config.AZURE_OPENAI_EMBEDDING_API_VERSION
        )
        
        logger.info(f"ğŸ”— ì„ë² ë”© API ë²„ì „: {config.AZURE_OPENAI_EMBEDDING_API_VERSION}")
        
        response = client.embeddings.create(
            model=config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
            input=text
        )
        return response.data[0].embedding
    except Exception as e:
        logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

# ===============================
# 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ (Azure AI Search)
# ===============================
@task(name="extract_text_from_document")
def extract_text_from_document(document_path: str, max_pages: int = None) -> Dict[str, Any]:
    """ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    logger = get_run_logger()
    logger.info(f"ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {document_path}")
    
    try:
        # PDF íŒŒì¼ ì—´ê¸°
        doc = fitz.open(document_path)
        total_pages = len(doc)  # ë¬¸ì„œ ë‹«ê¸° ì „ì— í˜ì´ì§€ ìˆ˜ ì €ì¥
        
        # ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ
        if max_pages and max_pages < total_pages:
            pages_to_process = max_pages
            logger.info(f"ğŸ“„ í˜ì´ì§€ ìˆ˜ ì œí•œ: {pages_to_process}/{total_pages} í˜ì´ì§€ë§Œ ì²˜ë¦¬")
        else:
            pages_to_process = total_pages
        
        extracted_text = {}
        
        for page_num in range(pages_to_process):
            page = doc.load_page(page_num)
            text = page.get_text()
            extracted_text[f"page_{page_num + 1}"] = {
                "text": text,
                "page_number": page_num + 1,
                "word_count": len(text.split())
            }
        
        doc.close()
        
        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_text)} í˜ì´ì§€")
        return {
            "document_path": document_path,
            "total_pages": total_pages,
            "extracted_text": extracted_text,
            "extraction_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        raise

# ===============================
# 2ë‹¨ê³„: í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜ ë° ì €ì¥
# ===============================
@task(name="capture_page_images")
def capture_page_images(document_path: str, output_dir: str = None, max_pages: int = None) -> Dict[str, Any]:
    """PDFì˜ ê° í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ìº¡ì²˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤."""
    logger = get_run_logger()
    logger.info(f"ï¸ í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜ ì‹œì‘: {document_path}")
    
    if output_dir is None:
        output_dir = config.OUTPUT_DIR
    
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    try:
        # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (í˜ì´ì§€ ìˆ˜ ì œí•œ)
        if max_pages:
            logger.info(f"ğŸ–¼ï¸ í˜ì´ì§€ ìˆ˜ ì œí•œ: ì²˜ìŒ {max_pages}í˜ì´ì§€ë§Œ ì´ë¯¸ì§€ ë³€í™˜")
            images = convert_from_path(document_path, dpi=300, first_page=1, last_page=max_pages)
        else:
            images = convert_from_path(document_path, dpi=300)
        
        image_paths = []
        document_name = Path(document_path).stem
        
        for i, image in enumerate(images):
            # í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ì €ì¥
            image_filename = f"{document_name}_page_{i+1}.png"
            image_path = output_path / image_filename
            image.save(image_path, "PNG", quality=95)
            image_paths.append(str(image_path))
            
            logger.info(f"ğŸ’¾ í˜ì´ì§€ {i+1} ì´ë¯¸ì§€ ì €ì¥: {image_path}")
        
        logger.info(f"âœ… ì´ë¯¸ì§€ ìº¡ì²˜ ì™„ë£Œ: {len(image_paths)}ê°œ í˜ì´ì§€")
        return {
            "document_path": document_path,
            "image_paths": image_paths,
            "output_directory": str(output_path),
            "capture_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ ì´ë¯¸ì§€ ìº¡ì²˜ ì‹¤íŒ¨: {str(e)}")
        raise

# ===============================
# 3ë‹¨ê³„: GPTë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± (ë³„ë„ API ë²„ì „ ì‚¬ìš©)
# ===============================
@task(name="generate_image_descriptions")
def generate_image_descriptions(image_paths: List[str]) -> Dict[str, Any]:
    """ì´ë¯¸ì§€ë“¤ì„ GPT Vision APIë¥¼ í†µí•´ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤. (GPT Vision ì „ìš© API ë²„ì „)"""
    logger = get_run_logger()
    logger.info(f"ğŸ¤– GPT ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹œì‘: {len(image_paths)}ê°œ ì´ë¯¸ì§€")
    
    try:
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ìƒˆë¡œìš´ API ë°©ì‹)
        client = openai.AzureOpenAI(
            azure_endpoint=config.AZURE_OPENAI_ENDPOINT,
            api_key=config.AZURE_OPENAI_KEY,
            api_version=config.AZURE_OPENAI_API_VERSION
        )
        
        logger.info(f"ğŸ”— GPT Vision API ë²„ì „: {config.AZURE_OPENAI_API_VERSION}")
        
        descriptions = {}
        
        for image_path in image_paths:
            try:
                # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
                with open(image_path, "rb") as image_file:
                    base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                
                # GPT Vision API í˜¸ì¶œ (ìƒˆë¡œìš´ API ë°©ì‹)
                response = client.chat.completions.create(
                    model=config.AZURE_OPENAI_DEPLOYMENT_NAME,
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": "ì´ ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸, ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ ë“± ëª¨ë“  ìš”ì†Œë¥¼ í¬í•¨í•˜ì—¬ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/png;base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ],
                    max_tokens=1000
                )
                
                description = response.choices[0].message.content
                descriptions[image_path] = {
                    "description": description,
                    "page_number": int(Path(image_path).stem.split('_')[-1].replace('page_', '')),
                    "generation_timestamp": datetime.now().isoformat()
                }
                
                logger.info(f"ğŸ“ í˜ì´ì§€ {descriptions[image_path]['page_number']} ì„¤ëª… ìƒì„± ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"âŒ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨ ({image_path}): {str(e)}")
                descriptions[image_path] = {
                    "description": f"ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {str(e)}",
                    "page_number": 0,
                    "generation_timestamp": datetime.now().isoformat()
                }
        
        logger.info(f"âœ… ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì™„ë£Œ: {len(descriptions)}ê°œ")
        return {
            "image_descriptions": descriptions,
            "total_images": len(image_paths),
            "generation_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ GPT ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {str(e)}")
        raise

# ===============================
# 4ë‹¨ê³„: Vector DB êµ¬ì„± (Azure OpenAI ì„ë² ë”© ì‚¬ìš©)
# ===============================
@task(name="create_vector_database")
def create_vector_database(
    extracted_text: Dict[str, Any], 
    image_descriptions: Dict[str, Any],
    document_path: str
) -> Dict[str, Any]:
    """ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„¤ëª…ì„ í•©ì³ì„œ Vector DBë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    logger = get_run_logger()
    logger.info(f"ğŸ—„ï¸ Vector DB êµ¬ì„± ì‹œì‘ (Azure OpenAI ì„ë² ë”© ì‚¬ìš©)")
    
    try:
        # Milvus Lite ì—°ê²°
        connections.connect("default", uri=config.MILVUS_URI)
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆë‹¤ë©´ ì‚­ì œ (ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°)
        collection_name = config.MILVUS_COLLECTION_NAME
        if utility.has_collection(collection_name):
            logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
            utility.drop_collection(collection_name)
        
        # ì»¬ë ‰ì…˜ ìŠ¤í‚¤ë§ˆ ì •ì˜ (í˜ì´ì§€ë³„ í†µí•© ë²¡í„°)
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="document_path", dtype=DataType.VARCHAR, max_length=500),
            FieldSchema(name="page_number", dtype=DataType.INT64),
            FieldSchema(name="content_type", dtype=DataType.VARCHAR, max_length=50),  # "combined"
            FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=15000),  # í†µí•© ì½˜í…ì¸ 
            FieldSchema(name="text_content", dtype=DataType.VARCHAR, max_length=10000),  # ì›ë³¸ í…ìŠ¤íŠ¸
            FieldSchema(name="image_description", dtype=DataType.VARCHAR, max_length=10000),  # ì´ë¯¸ì§€ ì„¤ëª…
            FieldSchema(name="image_path", dtype=DataType.VARCHAR, max_length=1000),  # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=3072)  # Azure OpenAI text-embedding-3-large
        ]
        
        schema = CollectionSchema(fields, "Document processing pipeline vector collection")
        
        # ì»¬ë ‰ì…˜ ìƒì„±
        collection = Collection(collection_name, schema)
        
        # Milvus Lite ìµœì í™” FLAT ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±
        index_params = {
            "metric_type": "COSINE",  # Azure OpenAI ì„ë² ë”©ì€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ìš©
            "index_type": "FLAT",     # Milvus Liteì—ì„œ ìµœê³  ì„±ëŠ¥
            "params": {}
        }
        collection.create_index("embedding", index_params)
        logger.info(f"ğŸ“š ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±: {collection_name} (3072ì°¨ì›)")
        
        # ë°ì´í„° ì¤€ë¹„ - í˜ì´ì§€ë³„ í†µí•© ë²¡í„° ë°©ì‹
        documents_to_insert = []
        embeddings_to_insert = []
        
        # í˜ì´ì§€ë³„ë¡œ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„¤ëª…ì„ í†µí•©
        page_data_map = {}
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ìˆ˜ì§‘
        for page_key, page_data in extracted_text["extracted_text"].items():
            page_num = page_data["page_number"]
            if page_num not in page_data_map:
                page_data_map[page_num] = {
                    "text_content": "",
                    "image_description": "",
                    "image_path": ""
                }
            if page_data["text"].strip():
                page_data_map[page_num]["text_content"] = page_data["text"][:10000]
        
        # ì´ë¯¸ì§€ ì„¤ëª… ë°ì´í„° ìˆ˜ì§‘
        for image_path, desc_data in image_descriptions["image_descriptions"].items():
            page_num = desc_data["page_number"]
            if page_num not in page_data_map:
                page_data_map[page_num] = {
                    "text_content": "",
                    "image_description": "",
                    "image_path": ""
                }
            if desc_data["description"].strip():
                page_data_map[page_num]["image_description"] = desc_data["description"][:10000]
                page_data_map[page_num]["image_path"] = image_path
        
        # í˜ì´ì§€ë³„ í†µí•© ì½˜í…ì¸  ìƒì„± ë° ë²¡í„°í™”
        for page_num, page_data in page_data_map.items():
            # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ì„¤ëª…ì„ ê²°í•©
            combined_content = ""
            if page_data["text_content"]:
                combined_content += f"í…ìŠ¤íŠ¸: {page_data['text_content']}"
            if page_data["image_description"]:
                if combined_content:
                    combined_content += " "
                combined_content += f"ì´ë¯¸ì§€: {page_data['image_description']}"
            
            if combined_content.strip():
                embedding = get_azure_openai_embedding(combined_content)
                
                documents_to_insert.append({
                    "document_path": document_path,
                    "page_number": page_num,
                    "content_type": "combined",  # í†µí•©ëœ ì½˜í…ì¸ 
                    "content": combined_content[:15000],  # ë” ê¸´ ê¸¸ì´ í—ˆìš©
                    "text_content": page_data["text_content"][:10000],
                    "image_description": page_data["image_description"][:10000],
                    "image_path": page_data["image_path"]
                })
                embeddings_to_insert.append(embedding)
        
        # ë°ì´í„° ì‚½ì…
        if documents_to_insert:
            # ì»¬ë ‰ì…˜ ë¡œë“œ
            collection.load()
            
            # ë°ì´í„° ì‚½ì…
            insert_data = [
                [doc["document_path"] for doc in documents_to_insert],
                [doc["page_number"] for doc in documents_to_insert],
                [doc["content_type"] for doc in documents_to_insert],
                [doc["content"] for doc in documents_to_insert],
                [doc["text_content"] for doc in documents_to_insert],
                [doc["image_description"] for doc in documents_to_insert],
                [doc["image_path"] for doc in documents_to_insert],
                embeddings_to_insert
            ]
            
            collection.insert(insert_data)
            collection.flush()
            
            logger.info(f"âœ… Vector DB êµ¬ì„± ì™„ë£Œ: {len(documents_to_insert)}ê°œ í•­ëª© ì‚½ì…")
        else:
            logger.warning("âš ï¸ ì‚½ì…í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return {
            "collection_name": config.MILVUS_COLLECTION_NAME,
            "total_documents": len(documents_to_insert),
            "combined_documents": len([d for d in documents_to_insert if d["content_type"] == "combined"]),
            "embedding_model": "Azure OpenAI text-embedding-3-large",
            "embedding_api_version": config.AZURE_OPENAI_EMBEDDING_API_VERSION,
            "embedding_dimension": 3072,
            "structure": "page_combined_vectors",  # í˜ì´ì§€ë³„ í†µí•© ë²¡í„° êµ¬ì¡°
            "creation_timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Vector DB êµ¬ì„± ì‹¤íŒ¨: {str(e)}")
        raise


# ===============================
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜ë“¤
# ===============================
@task(name="search_combined_vectors")
def search_combined_vectors(query: str, top_k: int = 5) -> Dict[str, Any]:
    """í†µí•© ë²¡í„°ì—ì„œ ê²€ìƒ‰ (í˜ì´ì§€ë³„ í†µí•© ê²€ìƒ‰)"""
    logger = get_run_logger()
    logger.info(f"ğŸ” í†µí•© ë²¡í„° ê²€ìƒ‰: {query}")
    
    try:
        # Milvus Lite ì—°ê²°
        connections.connect("default", uri=config.MILVUS_URI)
        collection = Collection(config.MILVUS_COLLECTION_NAME)
        collection.load()
        
        # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        query_embedding = get_azure_openai_embedding(query)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        search_params = {"metric_type": "COSINE", "params": {}}
        results = collection.search(
            [query_embedding],
            "embedding",
            search_params,
            limit=top_k,
            output_fields=["document_path", "page_number", "content_type", "content", 
                          "text_content", "image_description", "image_path"]
        )
        
        # ê²°ê³¼ ì •ë¦¬
        search_results = []
        for hits in results:
            for hit in hits:
                search_results.append({
                    "score": float(hit.score),
                    "document_path": hit.entity.get("document_path"),
                    "page_number": hit.entity.get("page_number"),
                    "content_type": hit.entity.get("content_type"),
                    "content": hit.entity.get("content"),
                    "text_content": hit.entity.get("text_content"),
                    "image_description": hit.entity.get("image_description"),
                    "image_path": hit.entity.get("image_path")
                })
        
        logger.info(f"âœ… í†µí•© ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
        return {
            "search_type": "combined_vectors",
            "query": query,
            "results": search_results,
            "total_results": len(search_results)
        }
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise


@task(name="search_text_only")
def search_text_only(query: str, top_k: int = 5) -> Dict[str, Any]:
    """í…ìŠ¤íŠ¸ ì½˜í…ì¸ ë§Œ ê²€ìƒ‰"""
    logger = get_run_logger()
    logger.info(f"ğŸ“ í…ìŠ¤íŠ¸ ì „ìš© ê²€ìƒ‰: {query}")
    
    try:
        # Milvus Lite ì—°ê²°
        connections.connect("default", uri=config.MILVUS_URI)
        collection = Collection(config.MILVUS_COLLECTION_NAME)
        collection.load()
        
        # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        query_embedding = get_azure_openai_embedding(query)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        search_params = {"metric_type": "COSINE", "params": {}}
        results = collection.search(
            [query_embedding],
            "embedding",
            search_params,
            limit=top_k,
            output_fields=["document_path", "page_number", "content_type", "content", 
                          "text_content", "image_description", "image_path"]
        )
        
        # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        search_results = []
        for hits in results:
            for hit in hits:
                text_content = hit.entity.get("text_content", "")
                if text_content.strip():  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    search_results.append({
                        "score": float(hit.score),
                        "document_path": hit.entity.get("document_path"),
                        "page_number": hit.entity.get("page_number"),
                        "content_type": "text_only",
                        "text_content": text_content,
                        "image_path": hit.entity.get("image_path")
                    })
        
        logger.info(f"âœ… í…ìŠ¤íŠ¸ ì „ìš© ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
        return {
            "search_type": "text_only",
            "query": query,
            "results": search_results,
            "total_results": len(search_results)
        }
        
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ì „ìš© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise


@task(name="search_image_only")
def search_image_only(query: str, top_k: int = 5) -> Dict[str, Any]:
    """ì´ë¯¸ì§€ ì„¤ëª…ë§Œ ê²€ìƒ‰"""
    logger = get_run_logger()
    logger.info(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì „ìš© ê²€ìƒ‰: {query}")
    
    try:
        # Milvus Lite ì—°ê²°
        connections.connect("default", uri=config.MILVUS_URI)
        collection = Collection(config.MILVUS_COLLECTION_NAME)
        collection.load()
        
        # ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        query_embedding = get_azure_openai_embedding(query)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        search_params = {"metric_type": "COSINE", "params": {}}
        results = collection.search(
            [query_embedding],
            "embedding",
            search_params,
            limit=top_k,
            output_fields=["document_path", "page_number", "content_type", "content", 
                          "text_content", "image_description", "image_path"]
        )
        
        # ì´ë¯¸ì§€ ì„¤ëª…ì´ ìˆëŠ” ê²°ê³¼ë§Œ í•„í„°ë§
        search_results = []
        for hits in results:
            for hit in hits:
                image_description = hit.entity.get("image_description", "")
                image_path = hit.entity.get("image_path", "")
                if image_description.strip() and image_path:  # ì´ë¯¸ì§€ ì„¤ëª…ê³¼ ê²½ë¡œê°€ ìˆëŠ” ê²½ìš°ë§Œ
                    search_results.append({
                        "score": float(hit.score),
                        "document_path": hit.entity.get("document_path"),
                        "page_number": hit.entity.get("page_number"),
                        "content_type": "image_only",
                        "image_description": image_description,
                        "image_path": image_path
                    })
        
        logger.info(f"âœ… ì´ë¯¸ì§€ ì „ìš© ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
        return {
            "search_type": "image_only",
            "query": query,
            "results": search_results,
            "total_results": len(search_results)
        }
        
    except Exception as e:
        logger.error(f"âŒ ì´ë¯¸ì§€ ì „ìš© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise


@task(name="hybrid_search")
def hybrid_search(query: str, top_k: int = 5, text_weight: float = 0.5, image_weight: float = 0.5) -> Dict[str, Any]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë³„ë„ ê²€ìƒ‰ í›„ ê²°ê³¼ í†µí•©"""
    logger = get_run_logger()
    logger.info(f"ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: {query} (í…ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜: {text_weight}, ì´ë¯¸ì§€ ê°€ì¤‘ì¹˜: {image_weight})")
    
    try:
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë³„ë„ë¡œ ê²€ìƒ‰
        text_results = search_text_only(query, top_k)
        image_results = search_image_only(query, top_k)
        
        # ê²°ê³¼ í†µí•© ë° ê°€ì¤‘ì¹˜ ì ìš©
        combined_results = []
        
        # í…ìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€
        for result in text_results["results"]:
            result["weighted_score"] = result["score"] * text_weight
            result["search_source"] = "text"
            combined_results.append(result)
        
        # ì´ë¯¸ì§€ ê²°ê³¼ ì¶”ê°€
        for result in image_results["results"]:
            result["weighted_score"] = result["score"] * image_weight
            result["search_source"] = "image"
            combined_results.append(result)
        
        # ê°€ì¤‘ì¹˜ ì ìˆ˜ë¡œ ì •ë ¬
        combined_results.sort(key=lambda x: x["weighted_score"], reverse=True)
        
        # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
        final_results = combined_results[:top_k]
        
        logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ í†µí•© ê²°ê³¼")
        return {
            "search_type": "hybrid",
            "query": query,
            "text_results_count": len(text_results["results"]),
            "image_results_count": len(image_results["results"]),
            "combined_results": final_results,
            "total_results": len(final_results),
            "weights": {"text": text_weight, "image": image_weight}
        }
        
    except Exception as e:
        logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise


def comprehensive_search(query: str) -> Dict[str, Any]:
    """í†µí•© ê²€ìƒ‰: 4ê°€ì§€ ê²€ìƒ‰ ë°©ì‹ì„ ëª¨ë‘ ì‹¤í–‰í•˜ê³  ì„¤ì •ì— ë”°ë¼ ê²°ê³¼ ë°˜í™˜"""
    logger.info(f"ğŸ” í†µí•© ê²€ìƒ‰ ì‹œì‘: '{query}'")
    
    try:
        ìƒ‰ = {
            "query": query,
            "search_config": {
                "combined_enabled": config.SEARCH_COMBINED,
                "text_only_enabled": config.SEARCH_TEXT_ONLY,
                "image_only_enabled": config.SEARCH_IMAGE_ONLY,
                "hybrid_enabled": config.SEARCH_HYBRID,
                "hybrid_weights": {
                    "text": config.HYBRID_TEXT_WEIGHT,
                    "image": config.HYBRID_IMAGE_WEIGHT
                },
                "top_k": config.SEARCH_TOP_K
            },
            "results": {}
        }
        
        # 1. í†µí•© ë²¡í„° ê²€ìƒ‰
        if config.SEARCH_COMBINED:
            logger.info("1ï¸âƒ£ í†µí•© ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰")
            try:
                combined_results = search_combined_vectors(query, config.SEARCH_TOP_K)
                search_results["results"]["combined"] = combined_results
                logger.info(f"âœ… í†µí•© ê²€ìƒ‰ ì™„ë£Œ: {combined_results['total_results']}ê°œ ê²°ê³¼")
            except Exception as e:
                logger.error(f"âŒ í†µí•© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                search_results["results"]["combined"] = {"error": str(e)}
        
        # 2. í…ìŠ¤íŠ¸ ì „ìš© ê²€ìƒ‰
        if config.SEARCH_TEXT_ONLY:
            logger.info("2ï¸âƒ£ í…ìŠ¤íŠ¸ ì „ìš© ê²€ìƒ‰ ì‹¤í–‰")
            try:
                text_results = search_text_only(query, config.SEARCH_TOP_K)
                search_results["results"]["text_only"] = text_results
                logger.info(f"âœ… í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì™„ë£Œ: {text_results['total_results']}ê°œ ê²°ê³¼")
            except Exception as e:
                logger.error(f"âŒ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                search_results["results"]["text_only"] = {"error": str(e)}
        
        # 3. ì´ë¯¸ì§€ ì „ìš© ê²€ìƒ‰
        if config.SEARCH_IMAGE_ONLY:
            logger.info("3ï¸âƒ£ ì´ë¯¸ì§€ ì „ìš© ê²€ìƒ‰ ì‹¤í–‰")
            try:
                image_results = search_image_only(query, config.SEARCH_TOP_K)
                search_results["results"]["image_only"] = image_results
                logger.info(f"âœ… ì´ë¯¸ì§€ ê²€ìƒ‰ ì™„ë£Œ: {image_results['total_results']}ê°œ ê²°ê³¼")
            except Exception as e:
                logger.error(f"âŒ ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                search_results["results"]["image_only"] = {"error": str(e)}
        
        # 4. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        if config.SEARCH_HYBRID:
            logger.info("4ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰")
            try:
                hybrid_results = hybrid_search(
                    query, 
                    config.SEARCH_TOP_K,
                    config.HYBRID_TEXT_WEIGHT,
                    config.HYBRID_IMAGE_WEIGHT
                )
                search_results["results"]["hybrid"] = hybrid_results
                logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {hybrid_results['total_results']}ê°œ ê²°ê³¼")
            except Exception as e:
                logger.error(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                search_results["results"]["hybrid"] = {"error": str(e)}
        
        # ê²°ê³¼ ìš”ì•½
        enabled_searches = []
        if config.SEARCH_COMBINED:
            enabled_searches.append("í†µí•©")
        if config.SEARCH_TEXT_ONLY:
            enabled_searches.append("í…ìŠ¤íŠ¸")
        if config.SEARCH_IMAGE_ONLY:
            enabled_searches.append("ì´ë¯¸ì§€")
        if config.SEARCH_HYBRID:
            enabled_searches.append("í•˜ì´ë¸Œë¦¬ë“œ")
        
        logger.info(f"ğŸ¯ ê²€ìƒ‰ ì™„ë£Œ: {', '.join(enabled_searches)} ê²€ìƒ‰ ì‹¤í–‰ë¨")
        
        return search_results
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        raise


# ===============================
# ë©”ì¸ íŒŒì´í”„ë¼ì¸ Flow
# ===============================
@flow(
    name="document_processing_pipeline",
    description="4ë‹¨ê³„ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸: í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ ì´ë¯¸ì§€ ìº¡ì²˜ â†’ GPT ì„¤ëª… â†’ Vector DB (ë¶„ë¦¬ëœ API ë²„ì „)",
    task_runner=ConcurrentTaskRunner()
)
def document_processing_pipeline(document_path: str, skip_image_processing: bool = False, max_pages: int = None, document_type: str = 'common'):
    """
    ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë©”ì¸ í•¨ìˆ˜
    
    Args:
        document_path: ì²˜ë¦¬í•  ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ
        skip_image_processing: ì´ë¯¸ì§€ ì²˜ë¦¬ ë‹¨ê³„ë¥¼ ê±´ë„ˆë›¸ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
        max_pages: ì²˜ë¦¬í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ê°’: None, ì „ì²´ í˜ì´ì§€ ì²˜ë¦¬)
    """
    logger = get_run_logger()
    logger.info(f" ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {document_path}")
    
    # í™˜ê²½ ë³€ìˆ˜ ê²€ì¦
    if not config.validate_config():
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # API ë²„ì „ ì •ë³´ ì¶œë ¥
    logger.info(f"ğŸ”— GPT Vision API ë²„ì „: {config.AZURE_OPENAI_API_VERSION}")
    logger.info(f"ğŸ”— ì„ë² ë”© API ë²„ì „: {config.AZURE_OPENAI_EMBEDDING_API_VERSION}")
    
    # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(document_path).exists():
        raise FileNotFoundError(f"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
    
    # 0ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
    logger.info("ğŸ—„ï¸ 0ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë° ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±")
    db_initialized = initialize_database()
    if not db_initialized:
        logger.warning("âš ï¸ PostgreSQL ì—°ê²° ì‹¤íŒ¨, ë©”íƒ€ë°ì´í„° ì €ì¥ ì—†ì´ ì§„í–‰")
    
    # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„±
    doc_metadata = None
    job_id = None
    if db_initialized:
        try:
            from prefect.context import get_run_context
            flow_run_context = get_run_context()
            flow_run_id = str(flow_run_context.flow_run.id) if flow_run_context and flow_run_context.flow_run else "unknown"
            
            doc_metadata = create_document_metadata(document_path, document_type)
            job_id = create_processing_job(doc_metadata["doc_id"], flow_run_id)
            logger.info(f"ğŸ“‹ ë¬¸ì„œ ID: {doc_metadata['doc_id']}, ì‘ì—… ID: {job_id}")
        except Exception as e:
            logger.warning(f"âš ï¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ìƒì„± ì‹¤íŒ¨, ê³„ì† ì§„í–‰: {str(e)}")
            db_initialized = False
    
    try:
        # 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if job_id:
            update_job_progress(job_id, "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘", 0)
        
        logger.info("ğŸ“„ 1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘")
        text_result = extract_text_from_document(document_path, max_pages)
        
        if job_id:
            update_job_progress(job_id, f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ - {text_result['total_pages']}í˜ì´ì§€", 1, 
                              {"extracted_pages": text_result['total_pages']})
        
        if skip_image_processing:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
            logger.info("â­ï¸ 2-3ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°")
            image_result = {"image_paths": []}
            description_result = {"image_descriptions": {}, "total_images": 0}
            
            if job_id:
                update_job_progress(job_id, "ì´ë¯¸ì§€ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°", 3)
                
        else:
            # 2ë‹¨ê³„: í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜
            if job_id:
                update_job_progress(job_id, "í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜ ì‹œì‘", 1)
                
            logger.info("ğŸ–¼ï¸ 2ë‹¨ê³„: í˜ì´ì§€ë³„ ì´ë¯¸ì§€ ìº¡ì²˜ ì‹œì‘")
            image_result = capture_page_images(document_path, max_pages=max_pages)
            
            if job_id:
                update_job_progress(job_id, f"ì´ë¯¸ì§€ ìº¡ì²˜ ì™„ë£Œ - {len(image_result['image_paths'])}ê°œ", 2,
                                  {"captured_images": len(image_result['image_paths'])})
            
            # 3ë‹¨ê³„: GPTë¥¼ ì´ìš©í•œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
            if job_id:
                update_job_progress(job_id, "GPT ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹œì‘", 2)
                
            logger.info("ğŸ¤– 3ë‹¨ê³„: GPT ì´ë¯¸ì§€ ì„¤ëª… ìƒì„± ì‹œì‘")
            description_result = generate_image_descriptions(image_result["image_paths"])
            
            if job_id:
                update_job_progress(job_id, f"GPT ì„¤ëª… ìƒì„± ì™„ë£Œ - {description_result['total_images']}ê°œ", 3,
                                  {"generated_descriptions": description_result['total_images']})
        
        # 4ë‹¨ê³„: Vector DB êµ¬ì„±
        if job_id:
            update_job_progress(job_id, "Vector DB êµ¬ì„± ì‹œì‘ (ì„ë² ë”© ìƒì„±)", 3)
            
        logger.info("ğŸ—„ï¸ 4ë‹¨ê³„: Vector DB êµ¬ì„± ì‹œì‘ (Azure OpenAI ì„ë² ë”©)")
        vector_result = create_vector_database(
            text_result, 
            description_result, 
            document_path
        )
        
        if job_id:
            update_job_progress(job_id, f"Vector DB êµ¬ì„± ì™„ë£Œ - {vector_result['total_documents']}ê°œ ë²¡í„°", 4,
                              {"vector_documents": vector_result['total_documents'],
                               "embedding_model": vector_result['embedding_model']})
        
        # 5ë‹¨ê³„: PostgreSQLì— ì²­í¬ ë°ì´í„° ì €ì¥
        saved_chunks = 0
        if db_initialized and doc_metadata and vector_result.get("total_documents", 0) > 0:
            logger.info("ğŸ’¾ 5ë‹¨ê³„: PostgreSQLì— ì²­í¬ ë°ì´í„° ì €ì¥")
            try:
                # Milvusì—ì„œ ë°©ê¸ˆ ì‚½ì…ëœ ë°ì´í„°ë¥¼ ì½ì–´ì™€ì„œ PostgreSQLì— ì €ì¥
                connections.connect("default", uri=config.MILVUS_URI)
                collection = Collection(config.MILVUS_COLLECTION_NAME)
                collection.load()
                
                # í˜„ì¬ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ ë°ì´í„° ì¡°íšŒ
                results = collection.query(
                    expr=f'document_path == "{document_path}"',
                    output_fields=["id", "document_path", "page_number", "content_type", 
                                 "content", "text_content", "image_description", "image_path"]
                )
                
                for doc_data in results:
                    chunk_data = {
                        "content_type": doc_data.get("content_type", "combined"),
                        "content": doc_data.get("content", ""),
                        "text_content": doc_data.get("text_content", ""),
                        "image_description": doc_data.get("image_description", ""),
                        "image_path": doc_data.get("image_path", ""),
                        "milvus_id": str(doc_data.get("id", ""))
                    }
                    
                    chunk_id = save_document_chunk(
                        doc_metadata["doc_id"], 
                        doc_data.get("page_number", 0), 
                        chunk_data
                    )
                    saved_chunks += 1
                    
                # ë¬¸ì„œ ì²˜ë¦¬ ìƒíƒœ ì—…ë°ì´íŠ¸
                update_document_processing_status(
                    doc_metadata["doc_id"], 
                    "completed",
                    total_pages=text_result['total_pages'],
                    processed_pages=text_result['total_pages'],
                    vector_count=vector_result['total_documents']
                )
                
                # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
                if job_id:
                    complete_processing_job(job_id, saved_chunks, vector_result['total_documents'])
                    
                logger.info(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ: {saved_chunks}ê°œ ì²­í¬")
                
            except Exception as e:
                logger.error(f"âŒ PostgreSQL ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                if doc_metadata and job_id:
                    try:
                        update_document_processing_status(doc_metadata["doc_id"], "failed", error_log=str(e))
                        complete_processing_job(job_id, saved_chunks, vector_result['total_documents'], str(e))
                    except:
                        pass
        
        # ê²°ê³¼ ìš”ì•½
        pipeline_result = {
            "document_path": document_path,
            "document_metadata": doc_metadata,
            "text_extraction": text_result,
            "image_capture": image_result,
            "image_descriptions": description_result,
            "vector_database": vector_result,
            "postgresql_storage": {
                "enabled": db_initialized,
                "saved_chunks": saved_chunks,
                "job_id": job_id
            },
            "pipeline_completion_time": datetime.now().isoformat(),
            "status": "success"
        }
        
        logger.info("âœ… ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        logger.info(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
        logger.info(f"   - ë¬¸ì„œ ID: {doc_metadata.get('doc_id', 'N/A') if doc_metadata else 'N/A'}")
        logger.info(f"   - ì´ í˜ì´ì§€ ìˆ˜: {text_result['total_pages']}")
        logger.info(f"   - ìº¡ì²˜ëœ ì´ë¯¸ì§€ ìˆ˜: {len(image_result['image_paths'])}")
        logger.info(f"   - ìƒì„±ëœ ì„¤ëª… ìˆ˜: {description_result['total_images']}")
        logger.info(f"   - Vector DB í•­ëª© ìˆ˜: {vector_result['total_documents']}")
        logger.info(f"   - PostgreSQL ì €ì¥ ì²­í¬ ìˆ˜: {saved_chunks}")
        logger.info(f"   - ì‚¬ìš©ëœ ì„ë² ë”© ëª¨ë¸: {vector_result['embedding_model']}")
        logger.info(f"   - ì„ë² ë”© API ë²„ì „: {vector_result['embedding_api_version']}")
        
        return pipeline_result
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return {
            "document_path": document_path,
            "status": "failed",
            "error": str(e),
            "failure_time": datetime.now().isoformat()
        }

# ===============================
# ì‹¤í–‰ ì˜ˆì œ
# ===============================
if __name__ == "__main__":
    # ì„¤ì • í™•ì¸
    config.print_config()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©
    import os
    test_document = os.getenv("TEST_DOCUMENT_PATH", "./test.pdf")
    
    if Path(test_document).exists():
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {test_document}")
        result = document_processing_pipeline(test_document)
        print(f" ì²˜ë¦¬ ê²°ê³¼: {result['status']}")
    else:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_document}")
